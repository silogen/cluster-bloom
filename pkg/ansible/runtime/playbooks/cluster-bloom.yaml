---
- name: Cluster Bloom - RKE2 Kubernetes Cluster Setup
  hosts: all
  become: yes
  gather_facts: true
  gather_subset:
    - min
    - network
  vars:
    FIRST_NODE: true
    GPU_NODE: true
    CONTROL_PLANE: false
    DOMAIN: ""
    CLUSTER_SIZE: small
    SERVER_IP: ""
    JOIN_TOKEN: ""
    NO_DISKS_FOR_CLUSTER: false
    CLUSTER_DISKS: []
    CLUSTER_PREMOUNTED_DISKS: ""
    USE_CERT_MANAGER: false
    CERT_OPTION: ""
    TLS_CERT: ""
    TLS_KEY: ""
    OIDC_URL: ""
    RKE2_EXTRA_CONFIG: ""
    CLUSTERFORGE_RELEASE: "none"
    SKIP_RANCHER_PARTITION_CHECK: false
    ADDITIONAL_TLS_SAN_URLS: ""
    PRELOAD_IMAGES: ""
    CF_VALUES: ""
    BLOOM_DIR: "~/.bloom"
    BLOOM_VERSION: "2.0.0"
    RKE2_VERSION: "v1.31.4+rke2r1"

    ROCM_BASE_URL: "https://repo.radeon.com/amdgpu-install/6.3.3/ubuntu/"
    ROCM_DEB_PACKAGE: "amdgpu-install_6.3.60303-1_all.deb"
    rke2_installation_url: "https://get.rke2.io"

    supported_ubuntu_versions:
      - "20.04"
      - "22.04"
      - "24.04"

    rke2_ports_tcp:
      - "80"
      - "443"
      - "2376"
      - "2379"
      - "2380"
      - "6443"
      - "9099"
      - "9345"
      - "10250"
      - "10254"
      - "30000:32767"

    rke2_ports_udp:
      - "8472"
      - "30000:32767"

    inotify_target_value: 512
    rancher_min_partition_gb: 500
    bloom_fstab_tag: "# managed by cluster-bloom"

  tasks:
    - name: Pre-deployment Data Safety Validation
      tags: [pre_deployment]
      include_tasks: tasks/data_safety_check.yml
      vars:
        validate_no_disks_for_cluster: "{{ NO_DISKS_FOR_CLUSTER | default(false) }}"
        validate_cluster_disks: "{{ CLUSTER_DISKS | default('') }}"
        validate_config_file: "{{ ansible_config_file | default('bloom.yaml') }}"

    - name: Node Validation
      tags: [validate_node]
      include_tasks: tasks/validate_node/main.yml

    - name: Node Preparation
      tags: [prepare_node]
      include_tasks: tasks/prepare_node/main.yml



















    - name: Deploy Cluster Tasks
      tags: [deploy_cluster]
      include_tasks: tasks/deploy_cluster/main.yml

    - name: Setup Node Annotator
      tags: [deploy_storage_classes, node-annotator, deploy_k8s_apps]
      when: FIRST_NODE
      include_tasks: tasks/setup_node_annotator.yml

    - name: Setup MetalLB (First Node)
      tags: [metallb, deploy_k8s_apps]
      when: FIRST_NODE
      block:
        - name: Get default IP for MetalLB
          shell: ip route get 1 | awk '{print $7; exit}'
          register: metallb_ip
          changed_when: false

        - name: Create MetalLB address pool config
          copy:
            content: |
              apiVersion: metallb.io/v1beta1
              kind: IPAddressPool
              metadata:
                name: cluster-bloom-ip-pool
                namespace: metallb-system
              spec:
                addresses:
                - {{ metallb_ip.stdout }}/32
              ---
              apiVersion: metallb.io/v1beta1
              kind: L2Advertisement
              metadata:
                name: cluster-bloom-l2-advertisement
                namespace: metallb-system
            dest: /var/lib/rancher/rke2/server/manifests/metallb-address.yaml
            mode: "0644"

    - name: Create Domain Configuration (First Node)
      tags: [domain, deploy_k8s_apps]
      when: FIRST_NODE and DOMAIN != ""
      block:
        - name: Wait for cluster to be ready
          pause:
            seconds: 5

        - name: Use generated certificates for ingress
          when: not USE_CERT_MANAGER and CERT_OPTION == "generate"
          block:
            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret from API server certificates
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert=/etc/rancher/rke2/certs/tls.crt \
                  --key=/etc/rancher/rke2/certs/tls.key \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
              register: secret_creation_result
              failed_when: secret_creation_result.rc != 0

        - name: Use existing certificate
          when: not USE_CERT_MANAGER and CERT_OPTION == "existing"
          block:
            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret from existing cert
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert={{ TLS_CERT }} \
                  --key={{ TLS_KEY }} \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

    - name: Preload Container Images
      tags: [images, deploy_k8s_apps]
      when: FIRST_NODE and PRELOAD_IMAGES is defined and PRELOAD_IMAGES != ""
      block:
        - name: Check for RKE2 containerd socket
          stat:
            path: /run/containerd/containerd.sock
          register: rke2_containerd_socket

        - name: Check for K3s containerd socket  
          stat:
            path: /run/k3s/containerd/containerd.sock
          register: k3s_containerd_socket

        - name: Determine containerd socket path
          set_fact:
            containerd_socket_path: "{{ '/run/containerd/containerd.sock' if rke2_containerd_socket.stat.exists else ('/run/k3s/containerd/containerd.sock' if k3s_containerd_socket.stat.exists else '') }}"
            containerd_runtime: "{{ 'RKE2' if rke2_containerd_socket.stat.exists else ('K3s' if k3s_containerd_socket.stat.exists else 'none') }}"

        - name: Fail if no containerd socket found
          fail:
            msg: |
              Error: No accessible containerd socket found.
              
              Checked locations:
                - RKE2: /run/containerd/containerd.sock ({{ 'FOUND' if rke2_containerd_socket.stat.exists else 'NOT FOUND' }})
                - K3s:  /run/k3s/containerd/containerd.sock ({{ 'FOUND' if k3s_containerd_socket.stat.exists else 'NOT FOUND' }})
              
              Please ensure RKE2 or K3s is installed and containerd is running:
                For RKE2: systemctl status containerd
                For K3s:  systemctl status k3s
                
              Verify socket permissions and that the container runtime is properly configured.
          when: containerd_socket_path == ''

        - name: Parse image list
          set_fact:
            image_list: "{{ PRELOAD_IMAGES.split(',') }}"

        - name: Report detected container runtime
          debug:
            msg: "Using {{ containerd_runtime }} containerd socket: {{ containerd_socket_path }}"

        - name: Pull container images
          shell: /var/lib/rancher/rke2/bin/ctr --address={{ containerd_socket_path }} --namespace k8s.io image pull {{ item }}
          loop: "{{ image_list }}"
          async: 3600
          poll: 0
          register: image_pull_jobs

        - name: Wait for image pulls to complete
          async_status:
            jid: "{{ item.ansible_job_id }}"
          loop: "{{ image_pull_jobs.results }}"
          register: image_pull_results
          until: image_pull_results.finished
          retries: 360
          delay: 10

      rescue:
        - name: Container image preloading failed
          fail:
            msg: |
              Container image preloading failed. This may be due to:
                1. Containerd not running properly (detected: {{ containerd_runtime }})
                2. Network connectivity issues preventing image downloads
                3. Invalid or incompatible image names in PRELOAD_IMAGES variable
                4. Insufficient disk space for image storage
              
              Note: For best compatibility, use fully qualified image names (e.g., 'docker.io/library/alpine:latest' instead of 'alpine:latest')
              
              Troubleshooting steps:
                1. Check container runtime status:
                   - RKE2: systemctl status containerd  
                   - K3s:  systemctl status k3s
                2. Verify socket: ls -la {{ containerd_socket_path }}
                3. Test manual image pull: /var/lib/rancher/rke2/bin/ctr --address={{ containerd_socket_path }} --namespace k8s.io image pull docker.io/library/alpine:latest
                4. Check network connectivity and DNS resolution
                5. Verify PRELOAD_IMAGES variable contains valid image references

    - name: Setup Storage Provisioner (Small/Medium Clusters - Local-Path)
      tags: [deploy_storage_classes, local-path, deploy_k8s_apps]
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER and CLUSTER_SIZE in ["small", "medium"]
      include_tasks: tasks/setup_local_path.yml

    - name: Setup Storage Provisioner (Large Clusters - Longhorn)  
      tags: [deploy_storage_classes, longhorn, deploy_k8s_apps]
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER and CLUSTER_SIZE == "large"
      include_tasks: tasks/setup_longhorn.yml

    - name: Create Bloom ConfigMap
      tags: [config, deploy_k8s_apps]
      when: FIRST_NODE
      block:
        - name: Get bloom version
          set_fact:
            bloom_version: "{{ BLOOM_VERSION | default('2.0.0') }}"

        - name: Create bloom config ConfigMap
          shell: |
            cat <<EOF | /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: bloom-config
              namespace: default
            data:
              version: "{{ bloom_version }}"
              gpu_node: "{{ GPU_NODE | lower }}"
              domain: "{{ DOMAIN }}"
              cluster_size: "{{ CLUSTER_SIZE }}"
              rke2_version: "{{ RKE2_VERSION }}"
            EOF

    - name: Setup ClusterForge
      tags: [clusterforge, deploy_clusterforge]
      when: FIRST_NODE and CLUSTERFORGE_RELEASE != "none"
      block:
        - name: Create ClusterForge directory in .bloom
          file:
            path: "{{ BLOOM_DIR }}/clusterforge"
            state: directory
            mode: "0755"

        - name: Download ClusterForge release
          get_url:
            url: "{{ CLUSTERFORGE_RELEASE }}"
            dest: "{{ BLOOM_DIR }}/clusterforge/clusterforge.tar.gz"
            mode: "0644"

        - name: Extract ClusterForge release
          unarchive:
            src: "{{ BLOOM_DIR }}/clusterforge/clusterforge.tar.gz"
            dest: "{{ BLOOM_DIR }}/clusterforge"
            remote_src: yes
            extra_opts: [--no-same-owner]

        - name: Run ClusterForge bootstrap script
          shell: |
            cd {{ BLOOM_DIR }}/clusterforge/cluster-forge/scripts
            {% if CF_VALUES != "" %}
            bash ./bootstrap.sh {{ DOMAIN }} {{ CF_VALUES }}
            {% else %}
            bash ./bootstrap.sh {{ DOMAIN }}
            {% endif %}
          register: cf_bootstrap
          environment:
            KUBECONFIG: /etc/rancher/rke2/rke2.yaml

        - name: Display ClusterForge deployment output
          debug:
            msg: "{{ cf_bootstrap.stdout }}"



  handlers:
    - name: Restart multipathd
      service:
        name: multipathd
        state: restarted

    - name: Create iptables directory
      ansible.builtin.file:
        path: /etc/iptables
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Save iptables
      shell: iptables-save > /etc/iptables/rules.v4
      ignore_errors: yes

    - name: Set permissions on iptables rules file
      ansible.builtin.file:
        path: /etc/iptables/rules.v4
        owner: root
        group: root
        mode: '0644'

    - name: Reload udev
      shell: |
        udevadm control --reload-rules
        udevadm trigger

    - name: Restart chronyd
      service:
        name: chronyd
        state: restarted

    - name: Restart rsyslog
      service:
        name: rsyslog
        state: restarted
