---
- name: Cluster Bloom - RKE2 Kubernetes Cluster Setup
  hosts: all
  become: yes
  vars:
    FIRST_NODE: true
    GPU_NODE: true
    CONTROL_PLANE: false
    DOMAIN: ""
    SERVER_IP: ""
    JOIN_TOKEN: ""
    NO_DISKS_FOR_CLUSTER: false
    CLUSTER_DISKS: []
    CLUSTER_PREMOUNTED_DISKS: ""
    USE_CERT_MANAGER: false
    CERT_OPTION: ""
    TLS_CERT: ""
    TLS_KEY: ""
    OIDC_URL: ""
    RKE2_EXTRA_CONFIG: ""
    CLUSTERFORGE_RELEASE: "none"

    rocm_base_url: "https://repo.radeon.com/amdgpu-install/6.3.3/ubuntu/"
    rocm_deb_package: "amdgpu-install_6.3.60303-1_all.deb"
    rke2_installation_url: "https://get.rke2.io"

    supported_ubuntu_versions:
      - "20.04"
      - "22.04"
      - "24.04"

    rke2_ports_tcp:
      - "80"
      - "443"
      - "2376"
      - "2379"
      - "2380"
      - "6443"
      - "9099"
      - "9345"
      - "10250"
      - "10254"
      - "30000:32767"

    rke2_ports_udp:
      - "8472"
      - "30000:32767"

    inotify_target_value: 512
    rancher_min_partition_gb: 500
    bloom_fstab_tag: "# managed by cluster-bloom"

  tasks:
    - name: Validate Ubuntu Version
      block:
        - name: Get Ubuntu version
          shell: grep VERSION_ID /etc/os-release | cut -d= -f2 | tr -d '"'
          register: ubuntu_version
          changed_when: false

        - name: Check if Ubuntu version is supported
          assert:
            that:
              - ubuntu_version.stdout in supported_ubuntu_versions
            fail_msg: "Ubuntu {{ ubuntu_version.stdout }} is not supported. Supported versions: {{ supported_ubuntu_versions | join(', ') }}"
            success_msg: "Running on supported Ubuntu {{ ubuntu_version.stdout }}"
      tags: [validate]

    - name: Install Dependent Packages
      block:
        - name: Update apt cache
          apt:
            update_cache: yes
          environment:
            DEBIAN_FRONTEND: noninteractive

        - name: Install required packages
          apt:
            name:
              - open-iscsi
              - jq
              - nfs-common
              - chrony
              - curl
              - wget
            state: present
          environment:
            DEBIAN_FRONTEND: noninteractive
            NEEDRESTART_MODE: a
            NEEDRESTART_SUSPEND: "1"
      tags: [packages]

    - name: Setup Multipath
      block:
        - name: Check if multipath.conf exists
          stat:
            path: /etc/multipath.conf
          register: multipath_conf

        - name: Create multipath.conf if not exists
          copy:
            content: |
              blacklist {
                  devnode "^sd[a-z0-9]+"
              }
            dest: /etc/multipath.conf
            mode: "0644"
          when: not multipath_conf.stat.exists

        - name: Ensure blacklist entry in multipath.conf
          blockinfile:
            path: /etc/multipath.conf
            block: |
              blacklist {
                  devnode "^sd[a-z0-9]+"
              }
            marker: "# {mark} ANSIBLE MANAGED BLOCK - cluster-bloom"
          when: multipath_conf.stat.exists
          notify: Restart multipathd
      tags: [storage]

    - name: Update Modprobe (GPU nodes)
      when: GPU_NODE
      block:
        - name: Un-blacklist amdgpu module
          shell: sed -i '/^blacklist amdgpu/s/^/# /' /etc/modprobe.d/*.conf
          ignore_errors: yes

        - name: Load amdgpu kernel module
          modprobe:
            name: amdgpu
            state: present
          ignore_errors: yes
      tags: [gpu]

    - name: Prepare Longhorn Disks
      when: not NO_DISKS_FOR_CLUSTER and CLUSTER_PREMOUNTED_DISKS == ""
      block:
        - name: Convert CLUSTER_DISKS from comma-separated string to list
          set_fact:
            cluster_disks_list: "{{ CLUSTER_DISKS.split(',') if CLUSTER_DISKS is string and CLUSTER_DISKS != '' else (CLUSTER_DISKS if CLUSTER_DISKS is sequence else []) }}"

        - name: Create mount points for cluster disks
          file:
            path: "/mnt/disk{{ item.0 }}"
            state: directory
            mode: "0755"
          loop: "{{ cluster_disks_list | default([]) | zip_longest([], fillvalue='') | list }}"
          loop_control:
            index_var: disk_index
          when: cluster_disks_list | length > 0

        - name: Format disks with ext4 (if not already formatted)
          shell: |
            if ! blkid {{ item }} | grep -q ext4; then
              wipefs -a {{ item }}
              mkfs.ext4 -F -F {{ item }}
            fi
          loop: "{{ cluster_disks_list | default([]) }}"
          when: cluster_disks_list | length > 0

        - name: Mount cluster disks
          mount:
            path: "/mnt/disk{{ item.0 }}"
            src: "{{ item.1 }}"
            fstype: ext4
            opts: defaults,nofail
            state: mounted
          loop: "{{ range(cluster_disks_list | length) | list | zip(cluster_disks_list) | list }}"
          when: cluster_disks_list | length > 0

        - name: Add mount entries to fstab with bloom tag
          lineinfile:
            path: /etc/fstab
            line: "UUID={{ ansible_facts['devices'][item.1 | basename]['links']['uuids'][0] }} /mnt/disk{{ item.0 }} ext4 defaults,nofail 0 2 {{ bloom_fstab_tag }}"
            state: present
          loop: "{{ range(cluster_disks_list | length) | list | zip(cluster_disks_list) | list }}"
          when: cluster_disks_list | length > 0
          ignore_errors: yes
      tags: [storage]

    - name: Prepare RKE2
      block:
        - name: Load required kernel modules
          modprobe:
            name: "{{ item }}"
            state: present
          loop:
            - iscsi_tcp
            - dm_mod

        - name: Ensure kernel modules load on boot
          lineinfile:
            path: /etc/modules-load.d/rke2.conf
            line: "{{ item }}"
            create: yes
          loop:
            - iscsi_tcp
            - dm_mod

        - name: Create RKE2 config directory
          file:
            path: /etc/rancher/rke2
            state: directory
            mode: "0755"

        - name: Create audit policy
          copy:
            content: |
              apiVersion: audit.k8s.io/v1
              kind: Policy
              metadata:
                creationTimestamp: null
              rules:
              - level: Metadata
            dest: /etc/rancher/rke2/audit-policy.yaml
            mode: "0644"

        - name: Create RKE2 config.yaml
          copy:
            content: |
              cni: cilium
              cluster-cidr: 10.242.0.0/16
              service-cidr: 10.243.0.0/16

              disable: rke2-ingress-nginx
              audit-log-path: "/var/lib/rancher/rke2/server/logs/kube-apiserver-audit.log"
              audit-log-maxage: 30
              audit-log-maxbackup: 10
              audit-log-maxsize: 100
              audit-policy-file: "/etc/rancher/rke2/audit-policy.yaml"
            dest: /etc/rancher/rke2/config.yaml
            mode: "0644"

        - name: Append extra RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: "{{ RKE2_EXTRA_CONFIG }}"
            marker: "# {mark} ANSIBLE MANAGED BLOCK - extra config"
          when: RKE2_EXTRA_CONFIG != ""

        - name: Handle OIDC configuration
          when: OIDC_URL != ""
          block:
            - name: Fetch OIDC certificate
              shell: |
                openssl s_client -showcerts -connect {{ OIDC_URL | regex_replace('^https?://', '') }}:443 </dev/null | \
                sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p'
              register: oidc_cert

            - name: Write OIDC certificate
              copy:
                content: "{{ oidc_cert.stdout }}"
                dest: /etc/rancher/rke2/oidc-ca.crt
                mode: "0644"

            - name: Append OIDC config to RKE2
              blockinfile:
                path: /etc/rancher/rke2/config.yaml
                block: |
                  kube-apiserver-arg:
                    - "--oidc-issuer-url={{ OIDC_URL }}"
                    - "--oidc-client-id=k8s"
                    - "--oidc-username-claim=preferred_username"
                    - "--oidc-groups-claim=groups"
                    - "--oidc-ca-file=/etc/rancher/rke2/oidc-ca.crt"
                    - "--oidc-username-prefix=oidc"
                    - "--oidc-groups-prefix=oidc"
                marker: "# {mark} ANSIBLE MANAGED BLOCK - OIDC"
      tags: [rke2]

    - name: Generate Node Labels
      blockinfile:
        path: /etc/rancher/rke2/config.yaml
        block: |
          node-label:
          {% if not NO_DISKS_FOR_CLUSTER %}
            - node.longhorn.io/create-default-disk=config
            - node.longhorn.io/instance-manager=true
          {% endif %}
            - cluster-bloom/gpu-node={{ GPU_NODE | lower }}
        marker: "# {mark} ANSIBLE MANAGED BLOCK - node labels"
      tags: [rke2]

    - name: Install Kubernetes Tools
      block:
        - name: Install k9s via snap
          snap:
            name: k9s
            state: present

        - name: Download yq
          get_url:
            url: https://github.com/mikefarah/yq/releases/download/v4.46.1/yq_linux_amd64
            dest: /usr/local/bin/yq
            mode: "0755"

        - name: Download kubectl
          get_url:
            url: https://dl.k8s.io/release/v1.34.2/bin/linux/amd64/kubectl
            dest: /usr/local/bin/kubectl
            mode: "0755"

        - name: Install Helm
          shell: |
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4 -o /tmp/get-helm-4.sh
            chmod +x /tmp/get-helm-4.sh
            DESIRED_VERSION=v4.0.0 /tmp/get-helm-4.sh
            rm /tmp/get-helm-4.sh
          args:
            creates: /usr/local/bin/helm
      tags: [tools]

    - name: Configure inotify instances
      block:
        - name: Get current inotify value
          shell: sysctl -n fs.inotify.max_user_instances
          register: current_inotify
          changed_when: false

        - name: Set inotify instances
          sysctl:
            name: fs.inotify.max_user_instances
            value: "{{ inotify_target_value }}"
            state: present
            sysctl_file: /etc/sysctl.conf
            reload: yes
          when: current_inotify.stdout | int < inotify_target_value
      tags: [system]

    - name: Open Firewall Ports
      block:
        - name: Open TCP ports
          iptables:
            chain: INPUT
            protocol: tcp
            destination_port: "{{ item }}"
            ctstate: NEW
            jump: ACCEPT
          loop: "{{ rke2_ports_tcp }}"
          notify: Save iptables

        - name: Open UDP ports
          iptables:
            chain: INPUT
            protocol: udp
            destination_port: "{{ item }}"
            ctstate: NEW
            jump: ACCEPT
          loop: "{{ rke2_ports_udp }}"
          notify: Save iptables
      tags: [firewall]

    - name: Update Udev Rules (GPU nodes)
      when: GPU_NODE
      block:
        - name: Create AMD GPU udev rules
          copy:
            content: |
              KERNEL=="kfd", MODE="0666"
              SUBSYSTEM=="drm", KERNEL=="renderD*", MODE="0666"
            dest: /etc/udev/rules.d/70-amdgpu.rules
            mode: "0644"
          notify: Reload udev

        - name: Reload udev rules immediately
          shell: |
            udevadm control --reload-rules
            udevadm trigger
      tags: [gpu]

    - name: Configure logrotate
      block:
        - name: Create iSCSI aggressive logrotate config
          copy:
            content: |
              # /etc/logrotate.d/iscsi-aggressive.conf
              /var/log/kern.log
              /var/log/syslog
              {
                  size 200M
                  rotate 10
                  compress
                  delaycompress
                  missingok
                  notifempty
                  create 0640 syslog adm
                  dateext
                  postrotate
                      /usr/lib/rsyslog/rsyslog-rotate
                  endscript
              }
            dest: /etc/logrotate.d/iscsi-aggressive.conf
            mode: "0644"

        - name: Create RKE2 logrotate config
          copy:
            content: |
              /var/lib/rancher/rke2/agent/containerd/containerd.log
              {
                  size 100M
                  rotate 10
                  compress
                  delaycompress
                  missingok
                  notifempty
                  create 0640 root root
                  dateext
              }
            dest: /etc/logrotate.d/rke2.conf
            mode: "0644"

        - name: Create logrotate cron job
          copy:
            content: |
              # Managed by Ansible - cluster-bloom
              SHELL=/bin/sh
              PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

              # iSCSI logrotate - runs every 10 minutes
              */10 * * * * root /usr/sbin/logrotate -f /etc/logrotate.d/iscsi-aggressive.conf >> /var/log/logrotate-bloom.log 2>&1

              # logrotate for RKE2 logs - runs hourly
              0 * * * * root /usr/sbin/logrotate -f /etc/logrotate.d/rke2.conf >> /var/log/logrotate-bloom.log 2>&1
            dest: /etc/cron.d/logrotate-bloom
            mode: "0644"

        - name: Ensure cron is running
          service:
            name: cron
            state: started
            enabled: yes
      tags: [logging]

    - name: Configure rsyslog rate limiting
      block:
        - name: Create rsyslog iSCSI filter
          copy:
            content: |
              # /etc/rsyslog.d/01-iscsi-filter.conf
              module(load="imuxsock" SysSockRateLimit.Interval="0")

              ruleset(name="iscsi_ratelimit") {
                  if ($msg contains "detected conn error") or
                  ($msg contains "session recovery timed out") or
                  ($msg contains "longhorn") then {
                      action(type="omfile"
                          file="/var/log/syslog"
                          dynaFile="default"
                          action.execOnlyEveryNthTime="500"
                          action.execOnlyEveryNthTimeTimeout="1200"
                      )
                      stop
                  }
              }
            dest: /etc/rsyslog.d/01-iscsi-filter.conf
            mode: "0644"
          notify: Restart rsyslog
      tags: [logging]

    - name: Setup RKE2 (First Node)
      when: FIRST_NODE
      block:
        - name: Install RKE2 server
          shell: curl -sfL {{ rke2_installation_url }} | sh -
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 server service
          service:
            name: rke2-server
            enabled: yes

        - name: Start RKE2 server service
          service:
            name: rke2-server
            state: started

        - name: Wait for RKE2 to be ready
          wait_for:
            path: /var/lib/rancher/rke2/server/node-token
            state: present
            timeout: 300
      tags: [rke2]

    - name: Setup RKE2 (Additional Node - Worker)
      when: not FIRST_NODE and not CONTROL_PLANE
      block:
        - name: Add server and token to RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: |
              server: https://{{ SERVER_IP }}:9345
              token: {{ JOIN_TOKEN }}
            marker: "# {mark} ANSIBLE MANAGED BLOCK - join config"

        - name: Install RKE2 agent
          shell: curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=agent sh -
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 agent service
          service:
            name: rke2-agent
            enabled: yes

        - name: Start RKE2 agent service
          service:
            name: rke2-agent
            state: started
      tags: [rke2]

    - name: Setup RKE2 (Additional Node - Control Plane)
      when: not FIRST_NODE and CONTROL_PLANE
      block:
        - name: Add server and token to RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: |
              server: https://{{ SERVER_IP }}:9345
              token: {{ JOIN_TOKEN }}
            marker: "# {mark} ANSIBLE MANAGED BLOCK - join config"

        - name: Install RKE2 server (control plane)
          shell: curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=server sh -
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 server service
          service:
            name: rke2-server
            enabled: yes

        - name: Start RKE2 server service
          service:
            name: rke2-server
            state: started
      tags: [rke2]

    - name: Setup KubeConfig (First Node)
      when: FIRST_NODE
      block:
        - name: Get main IP
          shell: ip route get 1.1.1.1 | awk '{print $7; exit}'
          register: main_ip
          changed_when: false

        - name: Update RKE2 kubeconfig with actual IP
          replace:
            path: /etc/rancher/rke2/rke2.yaml
            regexp: '127\.0\.0\.1'
            replace: "{{ main_ip.stdout }}"

        - name: Create .kube directory for user
          file:
            path: "/home/{{ ansible_user }}/.kube"
            state: directory
            mode: "0755"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"

        - name: Copy kubeconfig to user directory
          copy:
            src: /etc/rancher/rke2/rke2.yaml
            dest: "/home/{{ ansible_user }}/.kube/config"
            remote_src: yes
            mode: "0600"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"

        - name: Update PATH in bash_profile for k9s
          lineinfile:
            path: "/home/{{ ansible_user }}/.bash_profile"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: "{{ ansible_user }}"

        - name: Update PATH in bashrc for k9s
          lineinfile:
            path: "/home/{{ ansible_user }}/.bashrc"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: "{{ ansible_user }}"
      tags: [kubeconfig]

    - name: Create Chrony Config (First Node)
      when: FIRST_NODE
      block:
        - name: Backup original chrony.conf
          copy:
            src: /etc/chrony/chrony.conf
            dest: /etc/chrony/chrony.conf.bak
            remote_src: yes
          ignore_errors: yes

        - name: Create chrony.conf for first node
          copy:
            content: |
              pool 0.pool.ntp.org iburst maxsources 2
              server time.google.com iburst
              server time.cloudflare.com iburst

              pool pool.ntp.org iburst maxsources 4

              allow 10.0.0.0/8
            dest: /etc/chrony/chrony.conf
            mode: "0644"
          notify: Restart chronyd
      tags: [ntp]

    - name: Create Chrony Config (Additional Node)
      when: not FIRST_NODE and SERVER_IP != ""
      block:
        - name: Backup original chrony.conf
          copy:
            src: /etc/chrony/chrony.conf
            dest: /etc/chrony/chrony.conf.bak
            remote_src: yes
          ignore_errors: yes

        - name: Create chrony.conf for additional node
          copy:
            content: |
              pool pool.ntp.org iburst maxsources 4
              server time.google.com iburst
              server time.cloudflare.com iburst

              server {{ SERVER_IP }} iburst prefer

              pool 0.pool.ntp.org iburst maxsources 2
            dest: /etc/chrony/chrony.conf
            mode: "0644"
          notify: Restart chronyd
      tags: [ntp]

    - name: Setup Longhorn (First Node)
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER
      block:
        - name: Create RKE2 manifests directory
          file:
            path: /var/lib/rancher/rke2/server/manifests
            state: directory
            mode: "0755"

        - name: Wait for kubectl to be available
          wait_for:
            path: /var/lib/rancher/rke2/bin/kubectl
            state: present
            timeout: 300
      tags: [longhorn]

    - name: Setup MetalLB (First Node)
      when: FIRST_NODE
      block:
        - name: Get default IP for MetalLB
          shell: ip route get 1 | awk '{print $7; exit}'
          register: metallb_ip
          changed_when: false

        - name: Create MetalLB address pool config
          copy:
            content: |
              apiVersion: metallb.io/v1beta1
              kind: IPAddressPool
              metadata:
                name: cluster-bloom-ip-pool
                namespace: metallb-system
              spec:
                addresses:
                - {{ metallb_ip.stdout }}/32
              ---
              apiVersion: metallb.io/v1beta1
              kind: L2Advertisement
              metadata:
                name: cluster-bloom-l2-advertisement
                namespace: metallb-system
            dest: /var/lib/rancher/rke2/server/manifests/metallb-address.yaml
            mode: "0644"
      tags: [metallb]

    - name: Create Domain Configuration (First Node)
      when: FIRST_NODE and DOMAIN != ""
      block:
        - name: Wait for cluster to be ready
          pause:
            seconds: 5

        - name: Create DOMAIN ConfigMap
          shell: |
            cat <<EOF | /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: cluster-domain
              namespace: default
            data:
              DOMAIN: "{{ DOMAIN }}"
              use-cert-manager: "{{ USE_CERT_MANAGER | lower }}"
            EOF
          args:
            executable: /bin/bash

        - name: Generate self-signed certificate
          when: not USE_CERT_MANAGER and CERT_OPTION == "generate"
          block:
            - name: Generate TLS certificate
              shell: |
                openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
                  -keyout /tmp/tls.key \
                  -out /tmp/tls.crt \
                  -subj "/CN={{ DOMAIN }}" \
                  -addext "subjectAltName=DNS:{{ DOMAIN }},DNS:*.{{ DOMAIN }}"

            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert=/tmp/tls.crt \
                  --key=/tmp/tls.key \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Clean up temp cert files
              file:
                path: "{{ item }}"
                state: absent
              loop:
                - /tmp/tls.crt
                - /tmp/tls.key

        - name: Use existing certificate
          when: not USE_CERT_MANAGER and CERT_OPTION == "existing"
          block:
            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret from existing cert
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert={{ TLS_CERT }} \
                  --key={{ TLS_KEY }} \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
      tags: [DOMAIN]

    - name: Generate Join Command (First Node)
      when: FIRST_NODE
      block:
        - name: Get join token
          slurp:
            src: /var/lib/rancher/rke2/server/node-token
          register: JOIN_TOKEN_content

        - name: Get main IP
          shell: ip route get 1.1.1.1 | awk '{print $7; exit}'
          register: main_ip_final
          changed_when: false

        - name: Create additional node command file
          copy:
            content: |
              echo -e 'FIRST_NODE: false\nJOIN_TOKEN: {{ JOIN_TOKEN_content.content | b64decode | trim }}\nSERVER_IP: {{ main_ip_final.stdout }}' > bloom.yaml && sudo ./bloom --config bloom.yaml
            dest: "/tmp/additional_node_command.txt"
            mode: "0644"
          become: no

        - name: Display join information
          debug:
            msg: |
              ============================================
              Cluster setup complete!

              Join Token: {{ JOIN_TOKEN_content.content | b64decode | trim }}
              Server IP: {{ main_ip_final.stdout }}

              To add additional nodes, use:
              ansible-playbook cluster-bloom.yaml -e "FIRST_NODE=false SERVER_IP={{ main_ip_final.stdout }} JOIN_TOKEN={{ JOIN_TOKEN_content.content | b64decode | trim }}"
              ============================================
      tags: [output]

  handlers:
    - name: Restart multipathd
      service:
        name: multipathd
        state: restarted

    - name: Save iptables
      shell: iptables-save > /etc/iptables/rules.v4
      ignore_errors: yes

    - name: Reload udev
      shell: |
        udevadm control --reload-rules
        udevadm trigger

    - name: Restart chronyd
      service:
        name: chronyd
        state: restarted

    - name: Restart rsyslog
      service:
        name: rsyslog
        state: restarted
