---
- name: Cluster Bloom - RKE2 Kubernetes Cluster Setup
  hosts: all
  become: yes
  gather_facts: true
  gather_subset:
    - min
    - network
  vars:
    FIRST_NODE: true
    GPU_NODE: true
    CONTROL_PLANE: false
    DOMAIN: ""
    CLUSTER_SIZE: small
    SERVER_IP: ""
    JOIN_TOKEN: ""
    NO_DISKS_FOR_CLUSTER: false
    CLUSTER_DISKS: []
    CLUSTER_PREMOUNTED_DISKS: ""
    USE_CERT_MANAGER: false
    CERT_OPTION: ""
    TLS_CERT: ""
    TLS_KEY: ""
    OIDC_URL: ""
    RKE2_EXTRA_CONFIG: ""
    CLUSTERFORGE_RELEASE: "none"

    rocm_base_url: "https://repo.radeon.com/amdgpu-install/7.2/ubuntu/" 
    rocm_deb_package: "amdgpu-install_7.2.70200-1_all.deb"
    rke2_installation_url: "https://get.rke2.io"

    supported_ubuntu_versions:
      - "20.04"
      - "22.04"
      - "24.04"

    rke2_ports_tcp:
      - "80"
      - "443"
      - "2376"
      - "2379"
      - "2380"
      - "6443"
      - "9099"
      - "9345"
      - "10250"
      - "10254"
      - "30000:32767"

    rke2_ports_udp:
      - "8472"
      - "30000:32767"

    inotify_target_value: 512
    rancher_min_partition_gb: 500
    bloom_fstab_tag: "# managed by cluster-bloom"

  tasks:
    - name: Pre-deployment Data Safety Validation
      block:
        # Initialize validation issues list
        - name: Initialize validation issues list
          set_fact:
            validation_issues: []

        # RKE2 Installation Checks
        - name: Check if RKE2 server service exists and is running
          systemd:
            name: rke2-server
          register: rke2_server_service
          failed_when: false
          changed_when: false

        - name: Check if RKE2 agent service exists and is running
          systemd:
            name: rke2-agent
          register: rke2_agent_service
          failed_when: false
          changed_when: false

        - name: Check if RKE2 data directory exists
          stat:
            path: /var/lib/rancher/rke2
          register: rke2_data_dir

        - name: Check if RKE2 data directory is empty (if it exists)
          find:
            paths: /var/lib/rancher/rke2
            file_type: any
          register: rke2_data_contents
          when: rke2_data_dir.stat.exists

        - name: Check if RKE2 config directory exists
          stat:
            path: /etc/rancher/rke2
          register: rke2_config_dir

        - name: Check if RKE2 config directory is empty (if it exists)
          find:
            paths: /etc/rancher/rke2
            file_type: any
          register: rke2_config_contents
          when: rke2_config_dir.stat.exists

        # Collect RKE2 issues without failing
        - name: Collect RKE2 server service issue
          set_fact:
            validation_issues: "{{ validation_issues + ['RKE2 server service is running (' + rke2_server_service.status.ActiveState + ')'] }}"
          when: 
            - rke2_server_service.status is defined
            - rke2_server_service.status.ActiveState == "active"

        - name: Collect RKE2 agent service issue
          set_fact:
            validation_issues: "{{ validation_issues + ['RKE2 agent service is running (' + rke2_agent_service.status.ActiveState + ')'] }}"
          when:
            - rke2_agent_service.status is defined
            - rke2_agent_service.status.ActiveState == "active"

        - name: Collect RKE2 data directory issue
          set_fact:
            validation_issues: "{{ validation_issues + ['/var/lib/rancher/rke2 directory exists and contains ' + (rke2_data_contents.matched | string) + ' items'] }}"
          when:
            - rke2_data_dir.stat.exists
            - rke2_data_contents.matched > 0

        - name: Collect RKE2 config directory issue
          set_fact:
            validation_issues: "{{ validation_issues + ['/etc/rancher/rke2 directory exists and contains ' + (rke2_config_contents.matched | string) + ' items'] }}"
          when:
            - rke2_config_dir.stat.exists
            - rke2_config_contents.matched > 0

        # CLUSTER_DISKS Mount Checks (conditional)
        - name: Convert CLUSTER_DISKS from comma-separated string to list for validation
          set_fact:
            cluster_disks_validation_list: "{{ CLUSTER_DISKS.split(',') if CLUSTER_DISKS is string and CLUSTER_DISKS != '' else (CLUSTER_DISKS if CLUSTER_DISKS is sequence else []) }}"
          when: not NO_DISKS_FOR_CLUSTER and CLUSTER_DISKS != ""

        - name: Check mount status for each disk in CLUSTER_DISKS
          shell: "mount | grep -q '^{{ item }}' && echo 'mounted' || echo 'unmounted'"
          register: disk_mount_status
          loop: "{{ cluster_disks_validation_list | default([]) }}"
          when: not NO_DISKS_FOR_CLUSTER and CLUSTER_DISKS != "" and cluster_disks_validation_list | length > 0
          changed_when: false
          failed_when: false

        - name: Collect information about mounted disks
          set_fact:
            mounted_cluster_disks: "{{ mounted_cluster_disks | default([]) + [item.item] }}"
          loop: "{{ disk_mount_status.results | default([]) }}"
          when: not NO_DISKS_FOR_CLUSTER and CLUSTER_DISKS != "" and item.stdout is defined and item.stdout == 'mounted'

        - name: Collect CLUSTER_DISKS mount issue
          set_fact:
            validation_issues: "{{ validation_issues + ['CLUSTER_DISKS are mounted: ' + (mounted_cluster_disks | join(', '))] }}"
          when: not NO_DISKS_FOR_CLUSTER and CLUSTER_DISKS != "" and mounted_cluster_disks is defined and mounted_cluster_disks | length > 0

        # Report all issues at once
        - name: Fail deployment if any data safety issues found
          fail:
            msg: |
              ❌ Pre-deployment Data Safety Validation Failed
              
              The following issues were detected:
              {% for issue in validation_issues %}
              • {{ issue }}
              {% endfor %}
              
              These issues prevent safe deployment and could lead to data loss.
              
              Resolution:
              Use the --destroy-data flag to clean up all existing data:
              sudo ./bloom cli {{ ansible_config_file | default('bloom.yaml') }} --destroy-data
              
              WARNING: This will destroy ALL existing cluster and disk data!
          when: validation_issues | length > 0

        # Success message
        - name: Pre-deployment data safety validation completed
          debug:
            msg: "✓ Pre-deployment data safety validation completed successfully"
      tags: [validate_node, pre_deployment]

    - name: Validate Ubuntu Version
      block:
        - name: Get Ubuntu version
          shell: grep VERSION_ID /etc/os-release | cut -d= -f2 | tr -d '"'
          register: ubuntu_version
          changed_when: false

        - name: Check if Ubuntu version is supported
          assert:
            that:
              - ubuntu_version.stdout in supported_ubuntu_versions
            fail_msg: "Ubuntu {{ ubuntu_version.stdout }} is not supported. Supported versions: {{ supported_ubuntu_versions | join(', ') }}"
            success_msg: "Running on supported Ubuntu {{ ubuntu_version.stdout }}"
      tags: [validate_node]

    - name: Validate System Requirements
      block:
        - name: Check disk space on root partition
          shell: df -BG / | awk 'NR==2 {print $4}' | sed 's/G//'
          register: root_disk_space
          changed_when: false

        - name: Validate root partition has at least 20GB
          assert:
            that: root_disk_space.stdout | int >= 20
            fail_msg: "Root partition requires at least 20GB free space (found {{ root_disk_space.stdout }}GB)"

        - name: Check total memory
          shell: grep MemTotal /proc/meminfo | awk '{print int($2/1024/1024)}'
          register: total_memory
          changed_when: false

        - name: Validate minimum 4GB RAM
          assert:
            that: total_memory.stdout | int >= 4
            fail_msg: "System requires at least 4GB RAM (found {{ total_memory.stdout }}GB)"

        - name: Check CPU cores
          shell: grep -c ^processor /proc/cpuinfo
          register: cpu_cores
          changed_when: false

        - name: Validate minimum 2 CPU cores
          assert:
            that: cpu_cores.stdout | int >= 2
            fail_msg: "System requires at least 2 CPU cores (found {{ cpu_cores.stdout }})"

        - name: Check required kernel modules
          shell: |
            for mod in overlay br_netfilter; do
              if ! lsmod | grep -q "^$mod"; then
                if ! modinfo $mod >/dev/null 2>&1; then
                  echo "MISSING:$mod"
                  exit 1
                fi
              fi
            done
          register: kernel_modules_check
          failed_when: kernel_modules_check.rc != 0
          changed_when: false

      tags: [validate_node]

    - name: Check /var/lib/rancher partition size
      when: GPU_NODE and not SKIP_RANCHER_PARTITION_CHECK
      block:
        - name: Create /var/lib/rancher directory
          file:
            path: /var/lib/rancher
            state: directory
            mode: "0755"

        - name: Get partition size
          shell: df -BG /var/lib/rancher | awk 'NR==2 {print $2}' | sed 's/G//'
          register: rancher_partition_size
          changed_when: false

        - name: Warn if partition less than 500GB
          debug:
            msg: "WARNING: /var/lib/rancher partition is {{ rancher_partition_size.stdout }}GB, recommended 500GB for GPU nodes"
          when: rancher_partition_size.stdout | int < 500

        - name: Fail if partition less than 500GB
          fail:
            msg: "/var/lib/rancher partition size ({{ rancher_partition_size.stdout }}GB) is less than recommended 500GB"
          when: rancher_partition_size.stdout | int < 500
      tags: [validate_node]

    - name: Install Dependent Packages
      block:
        - name: Update apt cache
          apt:
            update_cache: yes
          environment:
            DEBIAN_FRONTEND: noninteractive
          ignore_errors: yes

        - name: Install required packages
          apt:
            name:
              - open-iscsi
              - jq
              - nfs-common
              - chrony
              - curl
              - wget
            state: present
          environment:
            DEBIAN_FRONTEND: noninteractive
            NEEDRESTART_MODE: a
            NEEDRESTART_SUSPEND: "1"
      tags: [packages, prep_node]

    - name: Setup Multipath
      block:
        - name: Check if multipath.conf exists
          stat:
            path: /etc/multipath.conf
          register: multipath_conf

        - name: Create multipath.conf if not exists
          copy:
            content: |
              blacklist {
                  devnode "^sd[a-z0-9]+"
              }
            dest: /etc/multipath.conf
            mode: "0644"
          when: not multipath_conf.stat.exists

        - name: Ensure blacklist entry in multipath.conf
          blockinfile:
            path: /etc/multipath.conf
            block: |
              blacklist {
                  devnode "^sd[a-z0-9]+"
              }
            marker: "# {mark} ANSIBLE MANAGED BLOCK - cluster-bloom"
          when: multipath_conf.stat.exists
          notify: Restart multipathd
      tags: [storage, prep_node]

    - name: Setup and Check ROCm
      when: GPU_NODE
      block:
        - name: Check if rocm-smi exists
          stat:
            path: /opt/rocm/bin/rocm-smi
          register: rocm_smi_check

        - name: Get Ubuntu codename
          shell: grep VERSION_CODENAME /etc/os-release | cut -d= -f2
          register: ubuntu_codename
          changed_when: false

        - name: Get kernel version
          shell: uname -r
          register: kernel_version
          changed_when: false

        - name: Install kernel headers and modules
          apt:
            name:
              - "linux-headers-{{ kernel_version.stdout }}"
              - "linux-modules-extra-{{ kernel_version.stdout }}"
              - python3-setuptools
              - python3-wheel
            state: present
          when: not rocm_smi_check.stat.exists
          environment:
            DEBIAN_FRONTEND: noninteractive
            NEEDRESTART_MODE: a
            NEEDRESTART_SUSPEND: "1"

        - name: Download amdgpu-install package
          get_url:
            url: "{{ ROCM_BASE_URL }}/{{ ubuntu_codename.stdout }}/{{ ROCM_DEB_PACKAGE }}"
            dest: "/tmp/{{ ROCM_DEB_PACKAGE }}"
            mode: "0644"
          when: not rocm_smi_check.stat.exists

        - name: Install amdgpu-install package
          apt:
            deb: "/tmp/{{ ROCM_DEB_PACKAGE }}"
            state: present
          when: not rocm_smi_check.stat.exists
          environment:
            DEBIAN_FRONTEND: noninteractive

        - name: Install ROCm
          shell: amdgpu-install --usecase=rocm,dkms --yes
          when: not rocm_smi_check.stat.exists
          environment:
            DEBIAN_FRONTEND: noninteractive

        - name: Load amdgpu module
          modprobe:
            name: amdgpu
            state: present

        - name: Verify GPUs with rocm-smi
          shell: rocm-smi -i --json | jq -r '.[] | .["Device Name"]' | sort | uniq -c
          register: rocm_smi_output
          changed_when: false

        - name: Display detected GPUs
          debug:
            msg: "ROCm Devices:\n{{ rocm_smi_output.stdout }}"

        - name: Validate GPU detection
          fail:
            msg: "No GPUs detected by rocm-smi"
          when: rocm_smi_output.stdout == ""

        - name: Read ROCm version
          slurp:
            src: /opt/rocm/.info/version
          register: rocm_version_file

        - name: Display ROCm version
          debug:
            msg: "ROCm version: {{ rocm_version_file.content | b64decode | trim }}"

      tags: [gpu, rocm, prep_node]

    - name: Update Modprobe (GPU nodes)
      when: GPU_NODE
      block:
        - name: Un-blacklist amdgpu module
          shell: sed -i '/^blacklist amdgpu/s/^/# /' /etc/modprobe.d/*.conf
          ignore_errors: yes

        - name: Load amdgpu kernel module
          modprobe:
            name: amdgpu
            state: present
          ignore_errors: yes
      tags: [gpu, prep_node]

    - name: Prepare Longhorn Disks
      when: not NO_DISKS_FOR_CLUSTER and CLUSTER_PREMOUNTED_DISKS == ""
      block:
        - name: Convert CLUSTER_DISKS from comma-separated string to list
          set_fact:
            cluster_disks_list: "{{ CLUSTER_DISKS.split(',') if CLUSTER_DISKS is string and CLUSTER_DISKS != '' else (CLUSTER_DISKS if CLUSTER_DISKS is sequence else []) }}"

        - name: Create mount points for cluster disks
          file:
            path: "/mnt/disk{{ disk_index }}"
            state: directory
            mode: "0755"
          loop: "{{ cluster_disks_list | default([]) }}"
          loop_control:
            index_var: disk_index
          when: cluster_disks_list | length > 0

        - name: Format disks with ext4 (if not already formatted)
          shell: |
            if ! blkid {{ item }} | grep -q ext4; then
              wipefs -a {{ item }}
              mkfs.ext4 -F -F {{ item }}
            fi
          loop: "{{ cluster_disks_list | default([]) }}"
          when: cluster_disks_list | length > 0

        - name: Get UUIDs for cluster disks
          shell: blkid -s UUID -o value {{ item.1 }}
          loop: "{{ range(cluster_disks_list | length) | list | zip(cluster_disks_list) | list }}"
          register: disk_uuids
          when: cluster_disks_list | length > 0
          changed_when: false

        - name: Add mount entries to fstab with bloom tag
          lineinfile:
            path: /etc/fstab
            line: "UUID={{ item.stdout }} /mnt/disk{{ item.item.0 }} ext4 defaults,nofail 0 2 {{ bloom_fstab_tag }}"
            state: present
          loop: "{{ disk_uuids.results }}"
          when: cluster_disks_list | length > 0 and not item.skipped | default(false)

        - name: Mount cluster disks using fstab entries (verifies fstab)
          shell: mount /mnt/disk{{ item.item.0 }}
          loop: "{{ disk_uuids.results }}"
          when: cluster_disks_list | length > 0 and not item.skipped | default(false)
          register: mount_results
          failed_when: mount_results.rc != 0
      tags: [storage, prep_node]

    - name: Prepare RKE2
      block:
        - name: Load required kernel modules
          modprobe:
            name: "{{ item }}"
            state: present
          loop:
            - iscsi_tcp
            - dm_mod

        - name: Ensure kernel modules load on boot
          lineinfile:
            path: /etc/modules-load.d/rke2.conf
            line: "{{ item }}"
            create: yes
          loop:
            - iscsi_tcp
            - dm_mod

        - name: Create RKE2 config directory
          file:
            path: /etc/rancher/rke2
            state: directory
            mode: "0755"

        - name: Create audit policy
          copy:
            content: |
              apiVersion: audit.k8s.io/v1
              kind: Policy
              metadata:
                creationTimestamp: null
              rules:
              - level: Metadata
            dest: /etc/rancher/rke2/audit-policy.yaml
            mode: "0644"

        - name: Get node IP for RKE2 cluster communication
          shell: >-
            ip -4 addr show | grep "inet 10.0.3\." | awk '{{ "{" }}print $2{{ "}" }}' | cut -d/ -f1 | head -1 || echo ""
          register: node_ip_result
          changed_when: false

        - name: Set node_ip fact
          set_fact:
            node_ip: "{{ node_ip_result.stdout if node_ip_result.stdout != '' else ansible_default_ipv4.address }}"

        - name: Create RKE2 config.yaml
          copy:
            content: |
              cni: cilium
              cluster-cidr: 10.242.0.0/16
              service-cidr: 10.243.0.0/16
              node-ip: {{ node_ip }}

              disable: rke2-ingress-nginx
              audit-log-path: "/var/lib/rancher/rke2/server/logs/kube-apiserver-audit.log"
              audit-log-maxage: 30
              audit-log-maxbackup: 10
              audit-log-maxsize: 100
              audit-policy-file: "/etc/rancher/rke2/audit-policy.yaml"
            dest: /etc/rancher/rke2/config.yaml
            mode: "0644"

        - name: Append extra RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: "{{ RKE2_EXTRA_CONFIG }}"
            marker: "# {mark} ANSIBLE MANAGED BLOCK - extra config"
          when: RKE2_EXTRA_CONFIG != ""

        - name: Handle TLS SAN configuration
          when: DOMAIN != "" or ADDITIONAL_TLS_SAN_URLS != ""
          block:
            - name: Initialize TLS SAN list
              set_fact:
                tls_san_list: []

            - name: Add auto-generated k8s SAN
              set_fact:
                tls_san_list: "{{ tls_san_list + ['k8s.' + DOMAIN] }}"
              when: DOMAIN != ""

            - name: Process additional TLS SANs
              set_fact:
                tls_san_list: "{{ tls_san_list + [item | trim] }}"
              loop: "{{ ADDITIONAL_TLS_SAN_URLS.split(',') }}"
              when: 
                - ADDITIONAL_TLS_SAN_URLS != ""
                - item | trim != ""

            - name: Generate TLS SAN block content
              set_fact:
                tls_san_block: |
                  tls-san:
                  {% for san in tls_san_list %}
                    - {{ san }}
                  {% endfor %}

            - name: Append TLS SAN configuration to RKE2 config
              blockinfile:
                path: /etc/rancher/rke2/config.yaml
                block: "{{ tls_san_block }}"
                marker: ""
              when: tls_san_list | length > 0

        - name: Generate API server certificates for Kubernetes
          when: not USE_CERT_MANAGER and CERT_OPTION == "generate" and DOMAIN != ""
          block:
            - name: Create RKE2 certificate directory
              file:
                path: /etc/rancher/rke2/certs
                state: directory
                mode: '0755'

            - name: Generate API server TLS certificate
              shell: |
                openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
                  -keyout /etc/rancher/rke2/certs/tls.key \
                  -out /etc/rancher/rke2/certs/tls.crt \
                  -subj "/CN={{ DOMAIN }}" \
                  -addext "subjectAltName=DNS:{{ DOMAIN }},DNS:*.{{ DOMAIN }}"
              register: cert_generation_result
              failed_when: cert_generation_result.rc != 0

        - name: Handle OIDC configuration
          when: OIDC_URL != ""
          block:
            - name: Fetch OIDC certificate
              shell: |
                openssl s_client -showcerts -connect {{ OIDC_URL | regex_replace('^https?://', '') }}:443 </dev/null | \
                sed -n '/-----BEGIN CERTIFICATE-----/,/-----END CERTIFICATE-----/p'
              register: oidc_cert

            - name: Write OIDC certificate
              copy:
                content: "{{ oidc_cert.stdout }}"
                dest: /etc/rancher/rke2/oidc-ca.crt
                mode: "0644"

            - name: Append OIDC config to RKE2
              blockinfile:
                path: /etc/rancher/rke2/config.yaml
                block: |
                  kube-apiserver-arg:
                    - "--oidc-issuer-url={{ OIDC_URL }}"
                    - "--oidc-client-id=k8s"
                    - "--oidc-username-claim=preferred_username"
                    - "--oidc-groups-claim=groups"
                    - "--oidc-ca-file=/etc/rancher/rke2/oidc-ca.crt"
                    - "--oidc-username-prefix=oidc"
                    - "--oidc-groups-prefix=oidc"
                marker: "# {mark} ANSIBLE MANAGED BLOCK - OIDC"

        - name: Create RKE2 manifests directory
          file:
            path: /var/lib/rancher/rke2/server/manifests
            state: directory
            mode: "0755"

        - name: Create Cilium HelmChartConfig for cluster size optimization
          copy:
            content: |
              apiVersion: helm.cattle.io/v1
              kind: HelmChartConfig
              metadata:
                name: rke2-cilium
                namespace: kube-system
              spec:
                valuesContent: |-
                  # Cilium Operator Configuration
                  operator:
                    # Set replicas based on cluster size
                    replicas: {{ '1' if CLUSTER_SIZE in ['small', 'medium'] else '2' }}
                    
                    # Resource limits appropriate for cluster size
                    resources:
                      limits:
                        cpu: {{ '500m' if CLUSTER_SIZE in ['small', 'medium'] else '1000m' }}
                        memory: {{ '512Mi' if CLUSTER_SIZE in ['small', 'medium'] else '1Gi' }}
                      requests:
                        cpu: {{ '50m' if CLUSTER_SIZE in ['small', 'medium'] else '100m' }}
                        memory: 128Mi
                  
                  # Hubble observability (disabled for all cluster sizes)
                  hubble:
                    enabled: false
                    relay:
                      enabled: false
                    ui:
                      enabled: false
                    metrics:
                      enabled: []
                  
                  # Network policy enforcement
                  policyEnforcement: "default"
                  
                  # IPAM configuration
                  ipam:
                    mode: kubernetes
            dest: /var/lib/rancher/rke2/server/manifests/rke2-cilium-config.yaml
            mode: "0644"

      tags: [rke2, deploy_cluster]

    - name: Generate Node Labels
      block:
        - name: Build disk labels list
          set_fact:
            disk_labels: "{{ disk_labels | default([]) + ['bloom.disk___mnt___disk' + item.0|string + '=disk' + item.1|replace('/', '___')] }}"
          loop: "{{ range(cluster_disks_list | default([]) | length) | list | zip(cluster_disks_list | default([])) | list }}"
          when: not NO_DISKS_FOR_CLUSTER and cluster_disks_list is defined and cluster_disks_list | length > 0

        - name: Write node labels to config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: |
              node-label:
              {% if not NO_DISKS_FOR_CLUSTER %}
                - node.longhorn.io/create-default-disk=config
                - node.longhorn.io/instance-manager=true
              {% endif %}
                - cluster-bloom/gpu-node={{ GPU_NODE | lower }}
              {% for label in disk_labels | default([]) %}
                - {{ label }}
              {% endfor %}
            marker: "# {mark} ANSIBLE MANAGED BLOCK - node labels"
      tags: [rke2, deploy_cluster]

    - name: Install Kubernetes Tools
      block:
        - name: Install k9s via snap
          snap:
            name: k9s
            state: present

        - name: Download yq
          get_url:
            url: https://github.com/mikefarah/yq/releases/download/v4.46.1/yq_linux_amd64
            dest: /usr/local/bin/yq
            mode: "0755"

        - name: Download kubectl
          get_url:
            url: https://dl.k8s.io/release/v1.34.2/bin/linux/amd64/kubectl
            dest: /usr/local/bin/kubectl
            mode: "0755"

        - name: Install Helm
          shell: |
            curl -fsSL https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-4 -o /tmp/get-helm-4.sh
            chmod +x /tmp/get-helm-4.sh
            DESIRED_VERSION=v4.0.0 /tmp/get-helm-4.sh
            rm /tmp/get-helm-4.sh
          args:
            creates: /usr/local/bin/helm
      tags: [tools, deploy_cluster]

    - name: Configure inotify instances
      block:
        - name: Get current inotify value
          shell: sysctl -n fs.inotify.max_user_instances
          register: current_inotify
          changed_when: false

        - name: Set inotify instances
          sysctl:
            name: fs.inotify.max_user_instances
            value: "{{ inotify_target_value }}"
            state: present
            sysctl_file: /etc/sysctl.conf
            reload: yes
          when: current_inotify.stdout | int < inotify_target_value
      tags: [system, prep_node]

    - name: Open Firewall Ports
      block:
        - name: Open TCP ports
          iptables:
            chain: INPUT
            protocol: tcp
            destination_port: "{{ item }}"
            ctstate: NEW
            jump: ACCEPT
          loop: "{{ rke2_ports_tcp }}"
          notify: Save iptables

        - name: Open UDP ports
          iptables:
            chain: INPUT
            protocol: udp
            destination_port: "{{ item }}"
            ctstate: NEW
            jump: ACCEPT
          loop: "{{ rke2_ports_udp }}"
          notify: Save iptables
      tags: [firewall, prep_node]

    - name: Update Udev Rules (GPU nodes)
      when: GPU_NODE
      block:
        - name: Create AMD GPU udev rules
          copy:
            content: |
              KERNEL=="kfd", MODE="0666"
              SUBSYSTEM=="drm", KERNEL=="renderD*", MODE="0666"
            dest: /etc/udev/rules.d/70-amdgpu.rules
            mode: "0644"
          notify: Reload udev

        - name: Reload udev rules immediately
          shell: |
            udevadm control --reload-rules
            udevadm trigger
      tags: [gpu, prep_node]

    - name: Configure logrotate
      block:
        - name: Create iSCSI aggressive logrotate config
          copy:
            content: |
              # /etc/logrotate.d/iscsi-aggressive.conf
              /var/log/kern.log
              /var/log/syslog
              {
                  size 200M
                  rotate 10
                  compress
                  delaycompress
                  missingok
                  notifempty
                  create 0640 syslog adm
                  dateext
                  postrotate
                      /usr/lib/rsyslog/rsyslog-rotate
                  endscript
              }
            dest: /etc/logrotate.d/iscsi-aggressive.conf
            mode: "0644"

        - name: Create RKE2 logrotate config
          copy:
            content: |
              /var/lib/rancher/rke2/agent/containerd/containerd.log
              {
                  size 100M
                  rotate 10
                  compress
                  delaycompress
                  missingok
                  notifempty
                  create 0640 root root
                  dateext
              }
            dest: /etc/logrotate.d/rke2.conf
            mode: "0644"

        - name: Create logrotate cron job
          copy:
            content: |
              # Managed by Ansible - cluster-bloom
              SHELL=/bin/sh
              PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin

              # iSCSI logrotate - runs every 10 minutes
              */10 * * * * root /usr/sbin/logrotate -f /etc/logrotate.d/iscsi-aggressive.conf >> /var/log/logrotate-bloom.log 2>&1

              # logrotate for RKE2 logs - runs hourly
              0 * * * * root /usr/sbin/logrotate -f /etc/logrotate.d/rke2.conf >> /var/log/logrotate-bloom.log 2>&1
            dest: /etc/cron.d/logrotate-bloom
            mode: "0644"

        - name: Ensure cron is running
          service:
            name: cron
            state: started
            enabled: yes
      tags: [logging, prep_node]

    - name: Configure rsyslog rate limiting
      block:
        - name: Create rsyslog iSCSI filter
          copy:
            content: |
              # /etc/rsyslog.d/01-iscsi-filter.conf
              module(load="imuxsock" SysSockRateLimit.Interval="0")

              ruleset(name="iscsi_ratelimit") {
                  if ($msg contains "detected conn error") or
                  ($msg contains "session recovery timed out") or
                  ($msg contains "longhorn") then {
                      action(type="omfile"
                          file="/var/log/syslog"
                          dynaFile="default"
                          action.execOnlyEveryNthTime="500"
                          action.execOnlyEveryNthTimeTimeout="1200"
                      )
                      stop
                  }
              }
            dest: /etc/rsyslog.d/01-iscsi-filter.conf
            mode: "0644"
          notify: Restart rsyslog
      tags: [logging, prep_node]

    - name: Setup RKE2 (First Node)
      when: FIRST_NODE
      block:
        - name: "Install RKE2 server{% if RKE2_VERSION is defined and RKE2_VERSION != '' %} ({{ RKE2_VERSION }}){% else %} (latest){% endif %}"
          shell: |
            {% if RKE2_VERSION is defined and RKE2_VERSION != "" %}
            curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_VERSION="{{ RKE2_VERSION }}" sh -
            {% else %}
            curl -sfL {{ rke2_installation_url }} | sh -
            {% endif %}
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 server service
          service:
            name: rke2-server
            enabled: yes

        - name: Start RKE2 server service
          shell: systemctl start rke2-server
          register: rke2_start
          failed_when: false
          changed_when: rke2_start.rc == 0

        - name: Wait for RKE2 to be ready
          wait_for:
            path: /var/lib/rancher/rke2/server/node-token
            state: present
            timeout: 300
      tags: [rke2, deploy_cluster]

    - name: Setup RKE2 (Additional Node - Worker)
      when: not FIRST_NODE and not CONTROL_PLANE
      block:
        - name: Add server and token to RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: |
              server: https://{{ SERVER_IP }}:9345
              token: {{ JOIN_TOKEN }}
            marker: "# {mark} ANSIBLE MANAGED BLOCK - join config"

        - name: "Install RKE2 agent{% if RKE2_VERSION is defined and RKE2_VERSION != '' %} ({{ RKE2_VERSION }}){% else %} (latest){% endif %}"
          shell: |
            {% if RKE2_VERSION is defined and RKE2_VERSION != "" %}
            curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=agent INSTALL_RKE2_VERSION="{{ RKE2_VERSION }}" sh -
            {% else %}
            curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=agent sh -
            {% endif %}
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 agent service
          service:
            name: rke2-agent
            enabled: yes

        - name: Start RKE2 agent service
          service:
            name: rke2-agent
            state: started
      tags: [rke2, deploy_cluster]

    - name: Setup RKE2 (Additional Node - Control Plane)
      when: not FIRST_NODE and CONTROL_PLANE
      block:
        - name: Add server and token to RKE2 config
          blockinfile:
            path: /etc/rancher/rke2/config.yaml
            block: |
              server: https://{{ SERVER_IP }}:9345
              token: {{ JOIN_TOKEN }}
            marker: "# {mark} ANSIBLE MANAGED BLOCK - join config"

        - name: "Install RKE2 server (control plane){% if RKE2_VERSION is defined and RKE2_VERSION != '' %} ({{ RKE2_VERSION }}){% else %} (latest){% endif %}"
          shell: |
            {% if RKE2_VERSION is defined and RKE2_VERSION != "" %}
            curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=server INSTALL_RKE2_VERSION="{{ RKE2_VERSION }}" sh -
            {% else %}
            curl -sfL {{ rke2_installation_url }} | INSTALL_RKE2_TYPE=server sh -
            {% endif %}
          args:
            creates: /usr/local/bin/rke2

        - name: Enable RKE2 server service
          service:
            name: rke2-server
            enabled: yes

        - name: Start RKE2 server service
          service:
            name: rke2-server
            state: started
      tags: [rke2, deploy_cluster]

    - name: Setup KubeConfig (Control Plane Nodes)
      when: FIRST_NODE or CONTROL_PLANE
      block:
        - name: Wait for RKE2 kubeconfig to be available
          wait_for:
            path: /etc/rancher/rke2/rke2.yaml
            state: present
            timeout: 300

        # Get home directories using getent
        - name: Get root home directory
          shell: "getent passwd root | cut -d: -f6"
          register: root_home
          changed_when: false

        - name: Get sudo user home directory
          shell: "getent passwd '{{ ansible_env.SUDO_USER }}' | cut -d: -f6"
          register: sudo_user_home
          changed_when: false
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""

        # Create kubeconfig for root user
        - name: Create .kube directory for root
          file:
            path: "{{ root_home.stdout }}/.kube"
            state: directory
            mode: "0755"
            owner: root
            group: root

        - name: Copy kubeconfig for root user
          copy:
            src: /etc/rancher/rke2/rke2.yaml
            dest: "{{ root_home.stdout }}/.kube/config"
            remote_src: yes
            mode: "0600"
            owner: root
            group: root

        - name: Update server IP in root kubeconfig
          replace:
            path: "{{ root_home.stdout }}/.kube/config"
            regexp: '127\.0\.0\.1'
            replace: "{{ node_ip }}"

        # Create kubeconfig for sudo user (if exists)
        - name: Create .kube directory for sudo user
          file:
            path: "{{ sudo_user_home.stdout }}/.kube"
            state: directory
            mode: "0755"
            owner: "{{ ansible_env.SUDO_USER }}"
            group: "{{ ansible_env.SUDO_USER }}"
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""

        - name: Copy kubeconfig for sudo user
          copy:
            src: /etc/rancher/rke2/rke2.yaml
            dest: "{{ sudo_user_home.stdout }}/.kube/config"
            remote_src: yes
            mode: "0600"
            owner: "{{ ansible_env.SUDO_USER }}"
            group: "{{ ansible_env.SUDO_USER }}"
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""

        - name: Update server IP in sudo user kubeconfig
          replace:
            path: "{{ sudo_user_home.stdout }}/.kube/config"
            regexp: '127\.0\.0\.1'
            replace: "{{ node_ip }}"
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""

        # Update PATH for k9s - root user
        - name: Update PATH in bash_profile for k9s (root)
          lineinfile:
            path: "{{ root_home.stdout }}/.bash_profile"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: root
            group: root

        - name: Update PATH in bashrc for k9s (root)
          lineinfile:
            path: "{{ root_home.stdout }}/.bashrc"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: root
            group: root

        # Update PATH for k9s - sudo user
        - name: Update PATH in bash_profile for k9s (sudo user)
          lineinfile:
            path: "{{ sudo_user_home.stdout }}/.bash_profile"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: "{{ ansible_env.SUDO_USER }}"
            group: "{{ ansible_env.SUDO_USER }}"
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""

        - name: Update PATH in bashrc for k9s (sudo user)
          lineinfile:
            path: "{{ sudo_user_home.stdout }}/.bashrc"
            line: "export PATH=$PATH:/snap/k9s/current/bin"
            create: yes
            owner: "{{ ansible_env.SUDO_USER }}"
            group: "{{ ansible_env.SUDO_USER }}"
          when: ansible_env.SUDO_USER is defined and ansible_env.SUDO_USER != ""
      tags: [kubeconfig, deploy_cluster]

    - name: Create Chrony Config (First Node)
      when: FIRST_NODE
      block:
        - name: Backup original chrony.conf
          copy:
            src: /etc/chrony/chrony.conf
            dest: /etc/chrony/chrony.conf.bak
            remote_src: yes
          ignore_errors: yes

        - name: Create chrony.conf for first node
          copy:
            content: |
              pool 0.pool.ntp.org iburst maxsources 2
              server time.google.com iburst
              server time.cloudflare.com iburst

              pool pool.ntp.org iburst maxsources 4

              allow 10.0.0.0/8
            dest: /etc/chrony/chrony.conf
            mode: "0644"
          notify: Restart chronyd
      tags: [ntp, prep_node]

    - name: Create Chrony Config (Additional Node)
      when: not FIRST_NODE and SERVER_IP != ""
      block:
        - name: Backup original chrony.conf
          copy:
            src: /etc/chrony/chrony.conf
            dest: /etc/chrony/chrony.conf.bak
            remote_src: yes
          ignore_errors: yes

        - name: Create chrony.conf for additional node
          copy:
            content: |
              pool pool.ntp.org iburst maxsources 4
              server time.google.com iburst
              server time.cloudflare.com iburst

              server {{ SERVER_IP }} iburst prefer

              pool 0.pool.ntp.org iburst maxsources 2
            dest: /etc/chrony/chrony.conf
            mode: "0644"
          notify: Restart chronyd
      tags: [ntp, prep_node]

    - name: Wait for kubectl to be available
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER
      wait_for:
        path: /var/lib/rancher/rke2/bin/kubectl
        state: present
        timeout: 300
      tags: [storage, deploy_k8s_apps]

    - name: Setup Node Annotator
      when: FIRST_NODE
      include_tasks: tasks/setup_node_annotator.yml
      tags: [node-annotator, deploy_k8s_apps]

    - name: Setup MetalLB (First Node)
      when: FIRST_NODE
      block:
        - name: Get default IP for MetalLB
          shell: ip route get 1 | awk '{print $7; exit}'
          register: metallb_ip
          changed_when: false

        - name: Create MetalLB address pool config
          copy:
            content: |
              apiVersion: metallb.io/v1beta1
              kind: IPAddressPool
              metadata:
                name: cluster-bloom-ip-pool
                namespace: metallb-system
              spec:
                addresses:
                - {{ metallb_ip.stdout }}/32
              ---
              apiVersion: metallb.io/v1beta1
              kind: L2Advertisement
              metadata:
                name: cluster-bloom-l2-advertisement
                namespace: metallb-system
            dest: /var/lib/rancher/rke2/server/manifests/metallb-address.yaml
            mode: "0644"
      tags: [metallb, deploy_k8s_apps]

    - name: Create Domain Configuration (First Node)
      when: FIRST_NODE and DOMAIN != ""
      block:
        - name: Wait for cluster to be ready
          pause:
            seconds: 5

        - name: Create DOMAIN ConfigMap
          shell: |
            cat <<EOF | /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: cluster-domain
              namespace: default
            data:
              DOMAIN: "{{ DOMAIN }}"
              use-cert-manager: "{{ USE_CERT_MANAGER | lower }}"
            EOF
          args:
            executable: /bin/bash

        - name: Use generated certificates for ingress
          when: not USE_CERT_MANAGER and CERT_OPTION == "generate"
          block:
            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret from API server certificates
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert=/etc/rancher/rke2/certs/tls.crt \
                  --key=/etc/rancher/rke2/certs/tls.key \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
              register: secret_creation_result
              failed_when: secret_creation_result.rc != 0

        - name: Use existing certificate
          when: not USE_CERT_MANAGER and CERT_OPTION == "existing"
          block:
            - name: Create kgateway-system namespace
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create namespace kgateway-system --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -

            - name: Create TLS secret from existing cert
              shell: |
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml \
                  create secret tls cluster-tls \
                  --cert={{ TLS_CERT }} \
                  --key={{ TLS_KEY }} \
                  -n kgateway-system \
                  --dry-run=client -o yaml | \
                /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
      tags: [DOMAIN, deploy_k8s_apps]

    - name: Preload Container Images
      when: FIRST_NODE and PRELOAD_IMAGES is defined and PRELOAD_IMAGES != ""
      block:
        - name: Check for RKE2 containerd socket
          stat:
            path: /run/containerd/containerd.sock
          register: rke2_containerd_socket

        - name: Check for K3s containerd socket  
          stat:
            path: /run/k3s/containerd/containerd.sock
          register: k3s_containerd_socket

        - name: Determine containerd socket path
          set_fact:
            containerd_socket_path: "{{ '/run/containerd/containerd.sock' if rke2_containerd_socket.stat.exists else ('/run/k3s/containerd/containerd.sock' if k3s_containerd_socket.stat.exists else '') }}"
            containerd_runtime: "{{ 'RKE2' if rke2_containerd_socket.stat.exists else ('K3s' if k3s_containerd_socket.stat.exists else 'none') }}"

        - name: Fail if no containerd socket found
          fail:
            msg: |
              Error: No accessible containerd socket found.
              
              Checked locations:
                - RKE2: /run/containerd/containerd.sock ({{ 'FOUND' if rke2_containerd_socket.stat.exists else 'NOT FOUND' }})
                - K3s:  /run/k3s/containerd/containerd.sock ({{ 'FOUND' if k3s_containerd_socket.stat.exists else 'NOT FOUND' }})
              
              Please ensure RKE2 or K3s is installed and containerd is running:
                For RKE2: systemctl status containerd
                For K3s:  systemctl status k3s
                
              Verify socket permissions and that the container runtime is properly configured.
          when: containerd_socket_path == ''

        - name: Parse image list
          set_fact:
            image_list: "{{ PRELOAD_IMAGES.split(',') }}"

        - name: Report detected container runtime
          debug:
            msg: "Using {{ containerd_runtime }} containerd socket: {{ containerd_socket_path }}"

        - name: Pull container images
          shell: /var/lib/rancher/rke2/bin/ctr --address={{ containerd_socket_path }} --namespace k8s.io image pull {{ item }}
          loop: "{{ image_list }}"
          async: 3600
          poll: 0
          register: image_pull_jobs

        - name: Wait for image pulls to complete
          async_status:
            jid: "{{ item.ansible_job_id }}"
          loop: "{{ image_pull_jobs.results }}"
          register: image_pull_results
          until: image_pull_results.finished
          retries: 360
          delay: 10

      rescue:
        - name: Container image preloading failed
          fail:
            msg: |
              Container image preloading failed. This may be due to:
                1. Containerd not running properly (detected: {{ containerd_runtime }})
                2. Network connectivity issues preventing image downloads
                3. Invalid or incompatible image names in PRELOAD_IMAGES variable
                4. Insufficient disk space for image storage
              
              Note: For best compatibility, use fully qualified image names (e.g., 'docker.io/library/alpine:latest' instead of 'alpine:latest')
              
              Troubleshooting steps:
                1. Check container runtime status:
                   - RKE2: systemctl status containerd  
                   - K3s:  systemctl status k3s
                2. Verify socket: ls -la {{ containerd_socket_path }}
                3. Test manual image pull: /var/lib/rancher/rke2/bin/ctr --address={{ containerd_socket_path }} --namespace k8s.io image pull docker.io/library/alpine:latest
                4. Check network connectivity and DNS resolution
                5. Verify PRELOAD_IMAGES variable contains valid image references
      
      tags: [images, deploy_k8s_apps]

    - name: Setup Storage Provisioner (Small/Medium Clusters - Local-Path)
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER and CLUSTER_SIZE in ["small", "medium"]
      include_tasks: tasks/setup_local_path.yml
      tags: [storage, local-path, deploy_k8s_apps]

    - name: Setup Storage Provisioner (Large Clusters - Longhorn)  
      when: FIRST_NODE and not NO_DISKS_FOR_CLUSTER and CLUSTER_SIZE == "large"
      include_tasks: tasks/setup_longhorn.yml
      tags: [storage, longhorn, deploy_k8s_apps]

    - name: Create Bloom ConfigMap
      when: FIRST_NODE
      block:
        - name: Get bloom version
          set_fact:
            bloom_version: "{{ BLOOM_VERSION | default('2.0.0') }}"

        - name: Create bloom config ConfigMap
          shell: |
            cat <<EOF | /var/lib/rancher/rke2/bin/kubectl --kubeconfig /etc/rancher/rke2/rke2.yaml apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: bloom-config
              namespace: default
            data:
              version: "{{ bloom_version }}"
              gpu_node: "{{ GPU_NODE | lower }}"
              domain: "{{ DOMAIN }}"
              cluster_size: "{{ CLUSTER_SIZE }}"
              rke2_version: "{{ RKE2_VERSION }}"
            EOF

      tags: [config, deploy_k8s_apps]

    - name: Setup ClusterForge
      when: FIRST_NODE and CLUSTERFORGE_RELEASE != "none"
      block:
        - name: Create ClusterForge directory in .bloom
          file:
            path: "{{ BLOOM_DIR }}/clusterforge"
            state: directory
            mode: "0755"

        - name: Download ClusterForge release
          get_url:
            url: "{{ CLUSTERFORGE_RELEASE }}"
            dest: "{{ BLOOM_DIR }}/clusterforge/clusterforge.tar.gz"
            mode: "0644"

        - name: Extract ClusterForge release
          unarchive:
            src: "{{ BLOOM_DIR }}/clusterforge/clusterforge.tar.gz"
            dest: "{{ BLOOM_DIR }}/clusterforge"
            remote_src: yes
            extra_opts: [--no-same-owner]

        - name: Run ClusterForge bootstrap script
          shell: |
            cd {{ BLOOM_DIR }}/clusterforge/cluster-forge/scripts
            {% if CF_VALUES != "" %}
            bash ./bootstrap.sh {{ DOMAIN }} {{ CF_VALUES }}
            {% else %}
            bash ./bootstrap.sh {{ DOMAIN }}
            {% endif %}
          register: cf_bootstrap
          environment:
            KUBECONFIG: /etc/rancher/rke2/rke2.yaml

        - name: Display ClusterForge deployment output
          debug:
            msg: "{{ cf_bootstrap.stdout }}"

      tags: [clusterforge, deploy_clusterforge]

    - name: Generate Join Command (First Node)
      when: FIRST_NODE
      block:
        - name: Get join token
          slurp:
            src: /var/lib/rancher/rke2/server/node-token
          register: JOIN_TOKEN_content

        - name: Create additional node command file
          copy:
            content: |
              echo -e 'FIRST_NODE: false\nJOIN_TOKEN: {{ JOIN_TOKEN_content.content | b64decode | trim }}\nSERVER_IP: {{ node_ip }}' > bloom.yaml && sudo ./bloom --config bloom.yaml
            dest: "{{ BLOOM_DIR }}/additional_node_command.txt"
            mode: "0644"
          become: no

        - name: Display join information
          debug:
            msg: |
              ============================================
              Cluster setup complete!

              Join Token: {{ JOIN_TOKEN_content.content | b64decode | trim }}
              Server IP: {{ node_ip }}

              To add additional nodes, use:
              ansible-playbook cluster-bloom.yaml -e "FIRST_NODE=false SERVER_IP={{ node_ip }} JOIN_TOKEN={{ JOIN_TOKEN_content.content | b64decode | trim }}"
              ============================================
      tags: [output, deploy_cluster]

  handlers:
    - name: Restart multipathd
      service:
        name: multipathd
        state: restarted

    - name: Create iptables directory
      ansible.builtin.file:
        path: /etc/iptables
        state: directory
        owner: root
        group: root
        mode: '0755'

    - name: Save iptables
      shell: iptables-save > /etc/iptables/rules.v4
      ignore_errors: yes

    - name: Set permissions on iptables rules file
      ansible.builtin.file:
        path: /etc/iptables/rules.v4
        owner: root
        group: root
        mode: '0644'

    - name: Reload udev
      shell: |
        udevadm control --reload-rules
        udevadm trigger

    - name: Restart chronyd
      service:
        name: chronyd
        state: restarted

    - name: Restart rsyslog
      service:
        name: rsyslog
        state: restarted
